{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "silent-grant",
   "metadata": {},
   "source": [
    "# Boston House Prices\n",
    "\n",
    "https://www.kaggle.com/vikrishnan/boston-house-prices?select=housing.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-burlington",
   "metadata": {},
   "source": [
    "Each record in the database describes a Boston suburb or town. The attributes are deﬁned as follows:\n",
    "- CRIM: per capita crime rate by town\n",
    "- ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- INDUS: proportion of non-retail business acres per town\n",
    "- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- NOX: nitric oxides concentration (parts per 10 million)\n",
    "- RM: average number of rooms per dwelling\n",
    "- AGE: proportion of owner-occupied units built prior to 1940\n",
    "- DIS: weighted distances to ﬁve Boston employment centers\n",
    "- RAD: index of accessibility to radial highways\n",
    "- TAX: full-value property-tax rate per 10,000\n",
    "- PTRATIO: pupil-teacher ratio by town \n",
    "- B: 1000(Bk−0.63)2 where Bk is the proportion of blacks by town \n",
    "- LSTAT: % lower status of the population\n",
    "- MEDV: Median value of owner-occupied homes in 1000s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-international",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fundamental-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caring-store",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var0</th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>var4</th>\n",
       "      <th>var5</th>\n",
       "      <th>var6</th>\n",
       "      <th>var7</th>\n",
       "      <th>var8</th>\n",
       "      <th>var9</th>\n",
       "      <th>var10</th>\n",
       "      <th>var11</th>\n",
       "      <th>var12</th>\n",
       "      <th>var13</th>\n",
       "      <th>var14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   var0     var1  var2  var3  var4   var5   var6  var7    var8  var9  var10  \\\n",
       "0     0  0.00632  18.0  2.31     0  0.538  6.575  65.2  4.0900     1    296   \n",
       "1     1  0.02731   0.0  7.07     0  0.469  6.421  78.9  4.9671     2    242   \n",
       "2     2  0.02729   0.0  7.07     0  0.469  7.185  61.1  4.9671     2    242   \n",
       "3     3  0.03237   0.0  2.18     0  0.458  6.998  45.8  6.0622     3    222   \n",
       "4     4  0.06905   0.0  2.18     0  0.458  7.147  54.2  6.0622     3    222   \n",
       "\n",
       "   var11   var12  var13  var14  \n",
       "0   15.3  396.90   4.98   24.0  \n",
       "1   17.8  396.90   9.14   21.6  \n",
       "2   17.8  392.83   4.03   34.7  \n",
       "3   18.7  394.63   2.94   33.4  \n",
       "4   18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Housing.csv', header=None, prefix=\"var\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liable-guatemala",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "olive-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into input and output values\n",
    "X = dataset.iloc[:, 1:-1]\n",
    "y = dataset.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "systematic-lawyer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>var4</th>\n",
       "      <th>var5</th>\n",
       "      <th>var6</th>\n",
       "      <th>var7</th>\n",
       "      <th>var8</th>\n",
       "      <th>var9</th>\n",
       "      <th>var10</th>\n",
       "      <th>var11</th>\n",
       "      <th>var12</th>\n",
       "      <th>var13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        var1  var2   var3  var4   var5   var6  var7    var8  var9  var10  \\\n",
       "0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900     1    296   \n",
       "1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671     2    242   \n",
       "2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671     2    242   \n",
       "3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622     3    222   \n",
       "4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622     3    222   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...   ...    ...   \n",
       "501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786     1    273   \n",
       "502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875     1    273   \n",
       "503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675     1    273   \n",
       "504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889     1    273   \n",
       "505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050     1    273   \n",
       "\n",
       "     var11   var12  var13  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  \n",
       "..     ...     ...    ...  \n",
       "501   21.0  391.99   9.67  \n",
       "502   21.0  396.90   9.08  \n",
       "503   21.0  396.90   5.64  \n",
       "504   21.0  393.45   6.48  \n",
       "505   21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vulnerable-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-virgin",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "original-blackberry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 13) (152, 13) (354,) (152,)\n"
     ]
    }
   ],
   "source": [
    "# In ANN neuron wights get multiplied with input, so it is necessary to scale the inputs to a common scale.\n",
    "# Also it helps in easier multiplication as I/Ps are scaled down\n",
    "# It also helps in back propogation as derivatives can be easily found with smaller values.\n",
    "# As a result, convergence will happen quickly.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-grass",
   "metadata": {},
   "source": [
    "### Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-shooting",
   "metadata": {},
   "source": [
    "I have defined a simple model with two hidden layers and an output layer that predicts a numeric value. I will use the ReLU activation function and “he” weight initialization, which are a good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "monetary-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "phantom-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(20, kernel_initializer='he_normal', activation='relu', input_dim=features))\n",
    "model.add(Dense(5, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acknowledged-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model and specify loss and optimizer\n",
    "opt = Adam(learning_rate=0.01, beta_1=0.85, beta_2=0.999)\n",
    "model.compile(optimizer=opt, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "engaging-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "25/25 [==============================] - 1s 17ms/step - loss: 637.0800 - val_loss: 444.6252\n",
      "Epoch 2/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 362.6816 - val_loss: 71.7220\n",
      "Epoch 3/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 54.3828 - val_loss: 34.0767\n",
      "Epoch 4/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 28.4038 - val_loss: 25.8543\n",
      "Epoch 5/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 30.1765 - val_loss: 22.5965\n",
      "Epoch 6/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 13.7853 - val_loss: 21.1756\n",
      "Epoch 7/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 15.8190 - val_loss: 18.6369\n",
      "Epoch 8/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 13.5980 - val_loss: 18.5499\n",
      "Epoch 9/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 11.3641 - val_loss: 17.2484\n",
      "Epoch 10/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 12.3517 - val_loss: 16.9188\n",
      "Epoch 11/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 12.4485 - val_loss: 16.3248\n",
      "Epoch 12/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 11.0516 - val_loss: 14.8625\n",
      "Epoch 13/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 8.5262 - val_loss: 13.9052\n",
      "Epoch 14/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 9.8878 - val_loss: 13.7936\n",
      "Epoch 15/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 7.4948 - val_loss: 13.7203\n",
      "Epoch 16/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 8.7044 - val_loss: 13.9524\n",
      "Epoch 17/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 9.5777 - val_loss: 13.0103\n",
      "Epoch 18/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 8.3220 - val_loss: 13.5336\n",
      "Epoch 19/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 9.0509 - val_loss: 12.8397\n",
      "Epoch 20/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 6.2829 - val_loss: 12.9156\n",
      "Epoch 21/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.8825 - val_loss: 12.4154\n",
      "Epoch 22/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 7.2472 - val_loss: 13.5840\n",
      "Epoch 23/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 6.8008 - val_loss: 12.1061\n",
      "Epoch 24/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 7.7913 - val_loss: 11.8755\n",
      "Epoch 25/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.5062 - val_loss: 14.1814\n",
      "Epoch 26/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 7.0600 - val_loss: 11.6701\n",
      "Epoch 27/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 7.8254 - val_loss: 10.9976\n",
      "Epoch 28/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 7.2841 - val_loss: 10.9436\n",
      "Epoch 29/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.1640 - val_loss: 11.7397\n",
      "Epoch 30/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 5.5865 - val_loss: 10.7696\n",
      "Epoch 31/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.8399 - val_loss: 12.9883\n",
      "Epoch 32/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 6.0948 - val_loss: 10.1316\n",
      "Epoch 33/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 7.9661 - val_loss: 12.7627\n",
      "Epoch 34/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.5121 - val_loss: 11.8218\n",
      "Epoch 35/250\n",
      "25/25 [==============================] - ETA: 0s - loss: 4.134 - 0s 3ms/step - loss: 5.8378 - val_loss: 10.2394\n",
      "Epoch 36/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.9190 - val_loss: 10.9809\n",
      "Epoch 37/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 7.2110 - val_loss: 10.1030\n",
      "Epoch 38/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.9463 - val_loss: 9.5065\n",
      "Epoch 39/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 6.4016 - val_loss: 10.3349\n",
      "Epoch 40/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 7.1589 - val_loss: 9.9836\n",
      "Epoch 41/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 6.3895 - val_loss: 9.5601\n",
      "Epoch 42/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.7515 - val_loss: 9.4134\n",
      "Epoch 43/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.3936 - val_loss: 11.5423\n",
      "Epoch 44/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.2260 - val_loss: 9.8715\n",
      "Epoch 45/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.6586 - val_loss: 10.4630\n",
      "Epoch 46/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.7481 - val_loss: 9.8762\n",
      "Epoch 47/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.4706 - val_loss: 11.2500\n",
      "Epoch 48/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.6869 - val_loss: 10.0462\n",
      "Epoch 49/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.8405 - val_loss: 10.2581\n",
      "Epoch 50/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.4394 - val_loss: 9.0662\n",
      "Epoch 51/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 6.4923 - val_loss: 9.6999\n",
      "Epoch 52/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.6127 - val_loss: 10.6992\n",
      "Epoch 53/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.4253 - val_loss: 9.1591\n",
      "Epoch 54/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.9938 - val_loss: 9.2248\n",
      "Epoch 55/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.1902 - val_loss: 9.5652\n",
      "Epoch 56/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.1164 - val_loss: 9.4984\n",
      "Epoch 57/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.6161 - val_loss: 9.8904\n",
      "Epoch 58/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.9079 - val_loss: 9.7111\n",
      "Epoch 59/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.4338 - val_loss: 9.0600\n",
      "Epoch 60/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.4956 - val_loss: 9.5823\n",
      "Epoch 61/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 5.0993 - val_loss: 11.1974\n",
      "Epoch 62/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.2990 - val_loss: 8.5787\n",
      "Epoch 63/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.3508 - val_loss: 10.1862\n",
      "Epoch 64/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 6.1490 - val_loss: 8.9296\n",
      "Epoch 65/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.1636 - val_loss: 9.2626\n",
      "Epoch 66/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.7210 - val_loss: 10.8307\n",
      "Epoch 67/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.4073 - val_loss: 8.9221\n",
      "Epoch 68/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.3226 - val_loss: 10.0208\n",
      "Epoch 69/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.1116 - val_loss: 10.4019\n",
      "Epoch 70/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 5.5000 - val_loss: 10.8418\n",
      "Epoch 71/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.7024 - val_loss: 10.3066\n",
      "Epoch 72/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.1381 - val_loss: 9.9545\n",
      "Epoch 73/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.8756 - val_loss: 11.1426\n",
      "Epoch 74/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.9298 - val_loss: 9.3986\n",
      "Epoch 75/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.5707 - val_loss: 10.2353\n",
      "Epoch 76/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.8751 - val_loss: 9.6017\n",
      "Epoch 77/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.2228 - val_loss: 12.3065\n",
      "Epoch 78/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 5.1850 - val_loss: 8.7631\n",
      "Epoch 79/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.5700 - val_loss: 10.5760\n",
      "Epoch 80/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.8364 - val_loss: 8.3431\n",
      "Epoch 81/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.9368 - val_loss: 8.7821\n",
      "Epoch 82/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 3ms/step - loss: 5.1340 - val_loss: 11.0355\n",
      "Epoch 83/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.6333 - val_loss: 9.2785\n",
      "Epoch 84/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.3909 - val_loss: 9.4130\n",
      "Epoch 85/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.1891 - val_loss: 9.4156\n",
      "Epoch 86/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.6075 - val_loss: 10.2818\n",
      "Epoch 87/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.7460 - val_loss: 10.8921\n",
      "Epoch 88/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.5057 - val_loss: 10.4754\n",
      "Epoch 89/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.6929 - val_loss: 8.7119\n",
      "Epoch 90/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 5.7408 - val_loss: 9.7666\n",
      "Epoch 91/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.1315 - val_loss: 10.2386\n",
      "Epoch 92/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.4395 - val_loss: 11.1963\n",
      "Epoch 93/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.3066 - val_loss: 11.0977\n",
      "Epoch 94/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.5450 - val_loss: 10.1437\n",
      "Epoch 95/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.4763 - val_loss: 9.5956\n",
      "Epoch 96/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.1736 - val_loss: 10.5659\n",
      "Epoch 97/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.6926 - val_loss: 12.7544\n",
      "Epoch 98/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.1634 - val_loss: 10.3846\n",
      "Epoch 99/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.4305 - val_loss: 11.6837\n",
      "Epoch 100/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.3280 - val_loss: 9.5224\n",
      "Epoch 101/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.2487 - val_loss: 10.5139\n",
      "Epoch 102/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.6737 - val_loss: 11.3064\n",
      "Epoch 103/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.3643 - val_loss: 11.6001\n",
      "Epoch 104/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.5062 - val_loss: 12.1509\n",
      "Epoch 105/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.3934 - val_loss: 8.6977\n",
      "Epoch 106/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.2200 - val_loss: 11.3099\n",
      "Epoch 107/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.7172 - val_loss: 9.9054\n",
      "Epoch 108/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.3929 - val_loss: 10.2711\n",
      "Epoch 109/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.4487 - val_loss: 9.5421\n",
      "Epoch 110/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.1062 - val_loss: 8.9798\n",
      "Epoch 111/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.6864 - val_loss: 9.2890\n",
      "Epoch 112/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.2062 - val_loss: 10.1307\n",
      "Epoch 113/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.8340 - val_loss: 9.4641\n",
      "Epoch 114/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.3202 - val_loss: 10.2284\n",
      "Epoch 115/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.3271 - val_loss: 9.8017\n",
      "Epoch 116/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.3166 - val_loss: 10.8019\n",
      "Epoch 117/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.6438 - val_loss: 9.4639\n",
      "Epoch 118/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.6860 - val_loss: 9.3563\n",
      "Epoch 119/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.7001 - val_loss: 10.4748\n",
      "Epoch 120/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.4780 - val_loss: 9.6606\n",
      "Epoch 121/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.1701 - val_loss: 11.0637\n",
      "Epoch 122/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.7938 - val_loss: 10.1720\n",
      "Epoch 123/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.7453 - val_loss: 10.8100\n",
      "Epoch 124/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.6843 - val_loss: 9.8064\n",
      "Epoch 125/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.6776 - val_loss: 8.6412\n",
      "Epoch 126/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.1842 - val_loss: 10.2909\n",
      "Epoch 127/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.4088 - val_loss: 11.5236\n",
      "Epoch 128/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.9380 - val_loss: 10.9856\n",
      "Epoch 129/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.5410 - val_loss: 9.7755\n",
      "Epoch 130/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.4919 - val_loss: 9.2259\n",
      "Epoch 131/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.6298 - val_loss: 10.5472\n",
      "Epoch 132/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.4391 - val_loss: 9.2811\n",
      "Epoch 133/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.0458 - val_loss: 11.2159\n",
      "Epoch 134/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.6781 - val_loss: 10.0267\n",
      "Epoch 135/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.6025 - val_loss: 10.9953\n",
      "Epoch 136/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.1993 - val_loss: 10.5292\n",
      "Epoch 137/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.2192 - val_loss: 10.0354\n",
      "Epoch 138/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.0019 - val_loss: 10.2986\n",
      "Epoch 139/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.0632 - val_loss: 9.1913\n",
      "Epoch 140/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.3776 - val_loss: 11.3756\n",
      "Epoch 141/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.3937 - val_loss: 11.2274\n",
      "Epoch 142/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.7682 - val_loss: 9.9448\n",
      "Epoch 143/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.3094 - val_loss: 11.0174\n",
      "Epoch 144/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.5120 - val_loss: 9.0876\n",
      "Epoch 145/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.4428 - val_loss: 11.4248\n",
      "Epoch 146/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.9193 - val_loss: 9.3652\n",
      "Epoch 147/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.7325 - val_loss: 9.5237\n",
      "Epoch 148/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.0814 - val_loss: 13.7018\n",
      "Epoch 149/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.3607 - val_loss: 9.5915\n",
      "Epoch 150/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.2230 - val_loss: 11.2696\n",
      "Epoch 151/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.8163 - val_loss: 9.6248\n",
      "Epoch 152/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.9773 - val_loss: 10.4453\n",
      "Epoch 153/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.3753 - val_loss: 12.5779\n",
      "Epoch 154/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.2267 - val_loss: 12.0713\n",
      "Epoch 155/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.4917 - val_loss: 9.4106\n",
      "Epoch 156/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.7419 - val_loss: 11.7568\n",
      "Epoch 157/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.3652 - val_loss: 12.4701\n",
      "Epoch 158/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.2542 - val_loss: 11.3491\n",
      "Epoch 159/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.4662 - val_loss: 15.7612\n",
      "Epoch 160/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.1532 - val_loss: 11.8206\n",
      "Epoch 161/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.8337 - val_loss: 11.2514\n",
      "Epoch 162/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.2841 - val_loss: 9.1560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.6139 - val_loss: 14.1345\n",
      "Epoch 164/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.4432 - val_loss: 10.0259\n",
      "Epoch 165/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.9752 - val_loss: 13.4262\n",
      "Epoch 166/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.2802 - val_loss: 8.9548\n",
      "Epoch 167/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.3604 - val_loss: 10.8905\n",
      "Epoch 168/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.8909 - val_loss: 10.0318\n",
      "Epoch 169/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.9234 - val_loss: 10.0334\n",
      "Epoch 170/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.4332 - val_loss: 13.8704\n",
      "Epoch 171/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.6322 - val_loss: 11.8573\n",
      "Epoch 172/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.8329 - val_loss: 12.6026\n",
      "Epoch 173/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.4342 - val_loss: 12.2607\n",
      "Epoch 174/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.9106 - val_loss: 10.7955\n",
      "Epoch 175/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.5547 - val_loss: 11.4590\n",
      "Epoch 176/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.1947 - val_loss: 10.9777\n",
      "Epoch 177/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.8697 - val_loss: 10.9947\n",
      "Epoch 178/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.9529 - val_loss: 11.5099\n",
      "Epoch 179/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.9698 - val_loss: 10.3365\n",
      "Epoch 180/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.8455 - val_loss: 10.1997\n",
      "Epoch 181/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.4888 - val_loss: 11.0964\n",
      "Epoch 182/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.0268 - val_loss: 10.8079\n",
      "Epoch 183/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.9552 - val_loss: 11.7355\n",
      "Epoch 184/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.1092 - val_loss: 12.7568\n",
      "Epoch 185/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.8864 - val_loss: 11.0170\n",
      "Epoch 186/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.7822 - val_loss: 10.7474\n",
      "Epoch 187/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.3819 - val_loss: 12.5085\n",
      "Epoch 188/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.0988 - val_loss: 12.4206\n",
      "Epoch 189/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.3295 - val_loss: 11.4523\n",
      "Epoch 190/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.6767 - val_loss: 10.7121\n",
      "Epoch 191/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.0286 - val_loss: 10.4788\n",
      "Epoch 192/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.4529 - val_loss: 12.2073\n",
      "Epoch 193/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.9937 - val_loss: 11.3045\n",
      "Epoch 194/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.5468 - val_loss: 9.9463\n",
      "Epoch 195/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.1673 - val_loss: 12.1280\n",
      "Epoch 196/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.2279 - val_loss: 10.4753\n",
      "Epoch 197/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.2208 - val_loss: 11.7556\n",
      "Epoch 198/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.8322 - val_loss: 11.3521\n",
      "Epoch 199/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.4651 - val_loss: 10.1857\n",
      "Epoch 200/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.7889 - val_loss: 12.9922\n",
      "Epoch 201/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.9129 - val_loss: 12.9046\n",
      "Epoch 202/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.8634 - val_loss: 10.8631\n",
      "Epoch 203/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.7501 - val_loss: 10.9888\n",
      "Epoch 204/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.8291 - val_loss: 10.2543\n",
      "Epoch 205/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.9403 - val_loss: 11.6864\n",
      "Epoch 206/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.7242 - val_loss: 10.6018\n",
      "Epoch 207/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.8968 - val_loss: 12.1716\n",
      "Epoch 208/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.4870 - val_loss: 12.2426\n",
      "Epoch 209/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.0040 - val_loss: 13.0381\n",
      "Epoch 210/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.7033 - val_loss: 10.5200\n",
      "Epoch 211/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.3404 - val_loss: 11.0916\n",
      "Epoch 212/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.8197 - val_loss: 13.5868\n",
      "Epoch 213/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.3906 - val_loss: 12.0818\n",
      "Epoch 214/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.8830 - val_loss: 12.5637\n",
      "Epoch 215/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.0121 - val_loss: 14.6679\n",
      "Epoch 216/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.1484 - val_loss: 19.3020\n",
      "Epoch 217/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.8131 - val_loss: 10.5474\n",
      "Epoch 218/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.1176 - val_loss: 11.1086\n",
      "Epoch 219/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.1040 - val_loss: 12.1800\n",
      "Epoch 220/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.1892 - val_loss: 11.4586\n",
      "Epoch 221/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.0415 - val_loss: 12.8105\n",
      "Epoch 222/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.4710 - val_loss: 10.7643\n",
      "Epoch 223/250\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.741 - 0s 3ms/step - loss: 2.7836 - val_loss: 10.3413\n",
      "Epoch 224/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.2111 - val_loss: 12.0439\n",
      "Epoch 225/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.1889 - val_loss: 11.4016\n",
      "Epoch 226/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.0152 - val_loss: 13.0625\n",
      "Epoch 227/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.8440 - val_loss: 10.9175\n",
      "Epoch 228/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.8786 - val_loss: 11.2434\n",
      "Epoch 229/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.9369 - val_loss: 13.1025\n",
      "Epoch 230/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.9709 - val_loss: 12.2041\n",
      "Epoch 231/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.8700 - val_loss: 12.1703\n",
      "Epoch 232/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.5197 - val_loss: 11.5694\n",
      "Epoch 233/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.6089 - val_loss: 12.1306\n",
      "Epoch 234/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.0565 - val_loss: 12.5941\n",
      "Epoch 235/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.0491 - val_loss: 11.1358\n",
      "Epoch 236/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.6628 - val_loss: 11.3840\n",
      "Epoch 237/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.2728 - val_loss: 12.0383\n",
      "Epoch 238/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.4412 - val_loss: 11.0991\n",
      "Epoch 239/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.4637 - val_loss: 14.2795\n",
      "Epoch 240/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.9169 - val_loss: 10.8321\n",
      "Epoch 241/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.7511 - val_loss: 12.6435\n",
      "Epoch 242/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.2845 - val_loss: 13.0979\n",
      "Epoch 243/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 2ms/step - loss: 3.1438 - val_loss: 13.7401\n",
      "Epoch 244/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.8546 - val_loss: 11.9522\n",
      "Epoch 245/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.2703 - val_loss: 11.1508\n",
      "Epoch 246/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.1285 - val_loss: 10.7392\n",
      "Epoch 247/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.8856 - val_loss: 12.2335\n",
      "Epoch 248/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.4145 - val_loss: 14.2903\n",
      "Epoch 249/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.0942 - val_loss: 13.0248\n",
      "Epoch 250/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.6618 - val_loss: 11.0630\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the training dataset\n",
    "model_history=model.fit(X_train, y_train,validation_split=0.30, epochs=250, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "middle-ceramic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    }
   ],
   "source": [
    "print(model_history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "geological-syndicate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize history for loss\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adolescent-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eight-abraham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 2.971\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average error in the predictions\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-slovak",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
